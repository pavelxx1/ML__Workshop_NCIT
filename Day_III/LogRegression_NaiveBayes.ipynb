{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will generate two dimensional data using the make blobs function and\n",
    "split it into training (2/3) and test set (1/3). We will then evaluate the result of Logistic regression with\n",
    "polynomial features where the degree of the polynomials is varied. We will also try to see hrough some tests whether logistic regression is prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import stats\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "\n",
    "std = 3\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std = std, random_state=42)\n",
    "\n",
    "#X, y = make_blobs(n_samples=500, centers=[[0,0],[5,5]], random_state=0, cluster_std = std)\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "# Plotting the cluster in different colors for easy visualization\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=y, cmap=cmap_bold);\n",
    "plt.title(\"CASE I : Random points with 2 classes (cluster_SD = %.3f)\"%std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing only the training points in plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.title(\"Training points extracted from the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the test points\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold)\n",
    "plt.title(\"Test points extracted from the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read more on Polynomial features here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "\n",
    "#### Read more on Pipeline here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Logistic regression \n",
    "deg = 1\n",
    "model = make_pipeline(PolynomialFeatures(deg),LogisticRegression(solver='lbfgs',C = 0.01))\n",
    "clf = model.fit(X_train, y_train)\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .1  # step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "plt.title(\"Decision surface of LogisticRegression\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Logistic regression with polynomial degree:  %.1f\" %deg)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the degree of polynomial to two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with polynomial of degree 2\n",
    "deg = 2\n",
    "model = make_pipeline(PolynomialFeatures(deg),LogisticRegression(solver='lbfgs',C = 0.01))\n",
    "clf = model.fit(X_train, y_train)\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .1  # step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "plt.title(\"Decision surface of LogisticRegression\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Logistic regression with polynomial degree:  %.1f\" %deg)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the degree of polynomial to three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with polynomial of degree 3\n",
    "deg = 3\n",
    "model = make_pipeline(PolynomialFeatures(deg),LogisticRegression(solver='lbfgs',C = 0.01)) # C is inverse of \n",
    "# regularization strength. Smaller values mean strong regularization\n",
    "clf = model.fit(X_train, y_train)\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .1  # step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "plt.title(\"Decision surface of LogisticRegression\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Logistic regression with polynomial degree:  %.1f\" %deg)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying the degrees of the polynomial to check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= {}\n",
    "\n",
    "for deg in range(1, 10, 1): # Varying the polynomial degree from 1 to 10 in step size of 1\n",
    "    l =[]\n",
    "    # Create an instance of classifier\n",
    "    model = make_pipeline(PolynomialFeatures(deg),LogisticRegression(solver='lbfgs',C = 0.01)) # C is inverse of \n",
    "    # regularization strength. Smaller values mean strong regularization\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    \n",
    "    # Fit the generated data to the model \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get the training scores\n",
    "    l.append(\"%.3f\"%clf.score(X_train, y_train))\n",
    "    \n",
    "    # Get the prediction result (for test data)\n",
    "    Z = clf.predict(X_test)\n",
    "\n",
    "    # Get the test accurracy\n",
    "    l.append(\"%.3f\"%accuracy_score(y_test, Z))\n",
    "    d[deg] = l\n",
    "\n",
    "# Printing out as a table format\n",
    "print (\"{:<8} {:<15} {:<10}\".format('Polynomial-degree','Training score','Test score'))\n",
    "for key, val in sorted(d.items()):\n",
    "    label, num = val\n",
    "    print (\"{:<18} {:<15} {:<10}\".format(key, label, num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit a Gaussian naive Bayes classifier to the Iris data set, thereby using\n",
    "2/3 of the data for training and 1/3 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) \n",
    "\n",
    "\n",
    "# create an instance of Gaussian Naive Bayes Classifier and fit the data.\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# GaussianNB(priors = None) # class priors are adapted from the data\n",
    "\n",
    "# centers of the Gaussians:\n",
    "centers = clf.theta_\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.scatter(centers[:,0],centers[:,1],s=150)  # gaussian centers as big blobs\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Gaussian naive Bayes classification with first two dimension (Iris)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, comparing it with the k-NN classifier, we see that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification with first two dimension(Iris) (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the last two features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) \n",
    "\n",
    "\n",
    "# create an instance of Gaussian Naive Bayes Classifier and fit the data.\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# GaussianNB(priors = None) # class priors are adapted from the data\n",
    "\n",
    "# centers of the Gaussians:\n",
    "centers = clf.theta_\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.scatter(centers[:,0],centers[:,1],s=150)  # gaussian centers as big blobs\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Gaussian naive Bayes classification with last two dimension (Iris)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, comparing it with the k-NN classifier, we see that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification with last two dimension(Iris) (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all four dimensions of the Iris dataset (splitting the dataset 2/3 training and 1/3 testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :]  # we take all four features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) \n",
    "\n",
    "\n",
    "# create an instance of Gaussian Naive Bayes Classifier and fit the data.\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# GaussianNB(priors = None) # class priors are adapted from the data\n",
    "\n",
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (clf.score(X_train, y_train)))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X_test)\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the result to k-NN neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10 # Number of neighbors for kNN\n",
    "\n",
    "# Create an instance of neighbors class imported from sklearn\n",
    "classifier = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "# Fit the generated data to the model \n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = classifier.predict(X_test)\n",
    "\n",
    "# print the training scores\n",
    "print(\"training score : %.3f\" % (classifier.score(X_train, y_train)))\n",
    "\n",
    "# print the test score\n",
    "print(\"prediction accuracy (test score): %.3f \" % accuracy_score(Z, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
