{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first use the method `makeblobs` from `sklearn.datasets.samples_generator` to\n",
    "generate three two-dimensional data sets with three classes where classes have a different degree of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First dataset with 3 clusters and cluster standard deviation = 0.85 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import stats\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "deviation_1 = 0.85\n",
    "\n",
    "X1, y1 = make_blobs(n_samples=500, centers=[[0,0],[-10,-10],[-15,15]],\n",
    "                  random_state=0, cluster_std=deviation_1)\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "# Plotting the cluster in different colors for easy visualization\n",
    "plt.scatter(X1[:, 0], X1[:, 1], s=20, c=y1, cmap=cmap_bold);\n",
    "plt.title(\"CASE I : Random points with 3 classes (cluster_SD = %f)\"%deviation_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see the classification error of a `5-NN classifier` for these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_neighbors = 5 # Number of neighbors for kNN\n",
    "\n",
    "# Create an instance of neighbors class imported from sklearn\n",
    "clf_1 = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "# Fit the generated data to the model \n",
    "clf_1.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "x1_min, x1_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\n",
    "y1_min, y1_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "xx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h),np.arange(y1_min, y1_max, h))\n",
    "\n",
    "# Get the prediction result\n",
    "Z1 = clf_1.predict(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z1 = Z1.reshape(xx1.shape)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx1, yy1, Z1, cmap=cmap_light)\n",
    "\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=y1, cmap=cmap_bold)\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(yy1.min(), yy1.max())\n",
    "plt.title(\" CASE I : 3-Class classification (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf_1.score(X1, y1))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf_1.predict(X1)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second dataset with 3 clusters and cluster standard deviation = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_2 = 1.0\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=200, centers=3,\n",
    "                  random_state=0, cluster_std=deviation_2)\n",
    "\n",
    "# Plotting the cluster for easy visualization\n",
    "plt.scatter(X2[:, 0], X2[:, 1], s=20, c=y2, cmap=cmap_bold);\n",
    "plt.title(\"CASE II : Random points with 3 classes (cluster_SD = %f)\"%deviation_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5 # Number of neighbors for kNN\n",
    "\n",
    "\n",
    "# Create an instance of neighbors class imported from sklearn\n",
    "clf_2 = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "# Fit the generated data to the model \n",
    "clf_2.fit(X2, y2)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "x2_min, x2_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\n",
    "y2_min, y2_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "xx2, yy2 = np.meshgrid(np.arange(x2_min, x2_max, h),np.arange(y2_min, y2_max, h))\n",
    "\n",
    "# Get the prediction result\n",
    "Z2 = clf_2.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z2 = Z2.reshape(xx2.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx2, yy2, Z2, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y2, cmap=cmap_bold)\n",
    "plt.xlim(xx2.min(), xx2.max())\n",
    "plt.ylim(yy2.min(), yy2.max())\n",
    "plt.title(\" CASE II : 3-Class classification (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf_2.score(X2, y2))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf_2.predict(X2)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third dataset with 3 clusters and cluster standard deviation = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_3 = 3\n",
    "\n",
    "X3, y3 = make_blobs(n_samples=500, centers=3,\n",
    "                  random_state=0, cluster_std=deviation_3)\n",
    "\n",
    "# Plotting the cluster for easy visualization\n",
    "plt.scatter(X3[:, 0], X3[:, 1], s=20, c=y3, cmap=cmap_bold);\n",
    "plt.title(\"CASE III : Random points with 3 classes (cluster_SD = %f)\"%deviation_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5 # Number of neighbors for kNN\n",
    "\n",
    "# Create an instance of neighbors class imported from sklearn\n",
    "clf_3 = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "# Fit the generated data to the model \n",
    "clf_3.fit(X3, y3)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "x3_min, x3_max = X3[:, 0].min() - 1, X3[:, 0].max() + 1\n",
    "y3_min, y3_max = X3[:, 1].min() - 1, X3[:, 1].max() + 1\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "xx3, yy3 = np.meshgrid(np.arange(x3_min, x3_max, h),np.arange(y3_min, y3_max, h))\n",
    "\n",
    "# Get the prediction result\n",
    "Z3 = clf_3.predict(np.c_[xx3.ravel(), yy3.ravel()])\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z3 = Z3.reshape(xx3.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx3, yy3, Z3, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=y3, cmap=cmap_bold)\n",
    "plt.xlim(xx3.min(), xx3.max())\n",
    "plt.ylim(yy3.min(), yy3.max())\n",
    "plt.title(\" CASE III : 3-Class classification (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf_3.score(X3, y3))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf_3.predict(X3)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate another data set by using `make_blobs(n_samples=200, centers=2, n_features=2, cluster_std = 5, random_state=42)` and split it into training (2/3 of the data) and test set (1/3 of the data). We will then evaluate the performance of a `k-NN classifier` as concerns training and test error for different choices of k, whereby the rank of k should be chosen such that an optimum choice of k can be inferred thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=200, centers=2, n_features =2, random_state=42, cluster_std=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=y, cmap=cmap_bold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the training points only in plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold)\n",
    "plt.title(\"Training points extracted from the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the test points\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold)\n",
    "plt.title(\"Test points extracted from the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= {}\n",
    "\n",
    "for k in range(5, 50, 5): # Varying the k value from 5 to 100 in step size of 10\n",
    "    l =[]\n",
    "    n_neighbors = k # Number of neighbors for kNN\n",
    "\n",
    "    # Create an instance of neighbors class imported from sklearn\n",
    "    classifier = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "    # Fit the generated data to the model \n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Get the training scores\n",
    "    l.append(\"%.3f\"%classifier.score(X_train, y_train))\n",
    "    \n",
    "    # Get the prediction result (for test data)\n",
    "    Z = classifier.predict(X_test)\n",
    "\n",
    "    # Get the test accurracy\n",
    "    l.append(\"%.3f\"%accuracy_score(y_test, Z))\n",
    "    d[k] = l\n",
    "\n",
    "# Printing out as a table format\n",
    "print (\"{:<8} {:<15} {:<10}\".format('k-value','Training score','Test score'))\n",
    "for key, val in sorted(d.items()):\n",
    "    label, num = val\n",
    "    print (\"{:<8} {:<15} {:<10}\".format(key, label, num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris data set is four dimensional; we have used only the first two\n",
    "dimensions for their classification in order to arrive at a direct visualization. Compare the\n",
    "result of a kNN classifier for the first two dimensions of the data to a k-NN classifier where\n",
    "all dimensions of the data are used. What happens if only the last two dimensions are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking only the first two dimension of the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "\n",
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification with first two dimension(Iris) (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf.score(X, y))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking only the last two dimensions of the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "\n",
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the last two features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification with last two dimensions (Iris) (k = %i)\" % n_neighbors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf.score(X, y))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all four dimensions of the Iris dataset (without splitting the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "\n",
    "# import data from IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :]  # we take all four features of the 4-D dataset \n",
    "y = iris.target\n",
    "\n",
    "# create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# print the training scores\n",
    "print(\"training score : %.3f \" % clf.score(X, y))\n",
    "\n",
    "# Get the prediction result (for test data)\n",
    "Z = clf.predict(X)\n",
    "\n",
    "# print the test accurracy\n",
    "print(\"prediction accuracy: %.3f \" % accuracy_score(Z, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all four dimensions of the Iris dataset (splitting the dataset 2/3 training and 1/3 testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting 33 percent (approx. 1/3) data for test and remaining 67 (approx. 2/3) for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) \n",
    "\n",
    "d= {}\n",
    "\n",
    "for i in range(5, 100, 10):\n",
    "    l =[]\n",
    "    n_neighbors = i # Number of neighbors for kNN\n",
    "\n",
    "    # Create an instance of neighbors class imported from sklearn\n",
    "    classifier = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "    # Fit the generated data to the model \n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Get the training scores\n",
    "    l.append(\"%.3f\"%classifier.score(X_train, y_train))\n",
    "    \n",
    "    # Get the prediction result (for test data)\n",
    "    Z = classifier.predict(X_test)\n",
    "\n",
    "    # Get the test accurracy\n",
    "    l.append(\"%.3f\"%accuracy_score(y_test, Z))\n",
    "    d[i] = l\n",
    "\n",
    "# Printing out as a table format\n",
    "print (\"{:<8} {:<15} {:<10}\".format('k-value','Training score','Test score'))\n",
    "for k, v in sorted(d.items()):\n",
    "    label, num = v\n",
    "    print (\"{:<8} {:<15} {:<10}\".format(k, label, num))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
